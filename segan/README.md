## Генеративная состязательная сеть: Segan
<p>GANs - это генеративные модели, которые учатся сопоставлять семплы z из некоторого предшествующего распределения Z, с семплами x из другого распределения X. X — набор данных (датасет) обучающих примеров (например, изображения, аудио и т. д.). Компонент в структуре GAN, выполняющий отображение, называется генератором (G), и его основная задача — изучить эффективное отображение, которое может имитировать реальное распределение данных для генерации новых семплов. Важно отметить то, что G делает это не путем запоминания пар "вход-выход", а путем нахождения функции, которая сопоставляет характеристики распределения исходных данных с характеристиками распределения Z.
Способ, которым G учится делать отображение, заключается в состязательном обучении, где у нас есть еще один компонент, называемый дискриминатором (D). D обычно является бинарным классификатором, и его входными данными являются либо реальные семплы, поступающие из набора данных, который имитирует G, либо поддельные семплы, составленные G. Состязательная характеристика исходит из того, что D должен классифицировать семплы, поступающие из X, как реальные, а семплы, поступающие из G долженклассифицировать как поддельные. Это приводит к тому, что G пытается обмануть D. G обучается, адаптируя свои параметры так, чтобы D классифицировал выходные семплы G, как реальные (см. рис. 1).
Задача улучшения зашумленнного сигнала определяется так, что есть входной зашумленный сигнал x, и мы хотим очистить его, чтобы получить улучшенный сигнал x'.
В нашем случае улучшение выполняет G-сеть. Его входы — шумный речевой сигнал x вместе с латентным представлением z, а выход-улучшенная версия x' = G(x). G — полностью сверточная нейросеть без полносвязных слоев. 
</p><p>Работа G состоит из двух этапов. Во-первых, происходит кодирование зашумленной речи в скрытое представление, и последующее декодирование скрытого представления и латентного вектора z  в очищенный от шумов сигнал. На этапе кодирования входной сигнал сжимается через ряд dilated сверточных слоев, за которыми следуют функции активации  PReLU. Используются dilated свертки, поскольку они более устойчивы для обучения GAN, чем другие подходы. Сжатие  выполняется до тех пор, пока не получается скрытое представление, называемое thought вектором, который соединяется с латентным вектором z. На этапе  декодирования  архитектура сети симметрична описанной выше.
Сеть G имеет skip-соединения: соединяет дополнительно каждый кодирующий слой с его симметричным декодирующим слоем и тем самым позволяет части информации протекать непосредственно между двумя связанными слоями, обходя сжатие, выполняемое в середине модели. Без skip-соединений часть информации в узком месте сети может быть потеряна. В декодере полученной на вход информации может быть недостаточно для правильного восстановления сигнала. Через skip-соединения передается информация о форме сигнала, чтобы реконструирующая часть сетки (декодер) лучше смогла восстановить аудиосигнал (см. рис. 2).
D отвечает за передачу информации G о том, что реально, а что фальшиво. G может слегка скорректировать свою выходную форму сигнала в сторону правильного распределения. Он избавляется от шумных сигналов, поскольку они определяются D, как фальшивые. В этом смысле D можно понимать, как функцию потерь для G.</p>
<p align = 'center'><img src ='segan1.png' alt=" Схема состязательного обучения" width="50%"></p>
<p align = 'center'><img src ='segan2.png' alt=" Архитектура автоэнкодера. Стрелки отображают соединения между кодирующим с ним симметричным декодирующим слоями" width="50%"></p>

Источник: <a href = 'https://arxiv.org/pdf/1703.09452.pdf'>Можете здесь посмотреть исходный текст статьи</a>
